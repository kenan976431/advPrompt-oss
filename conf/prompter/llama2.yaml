defaults:
  - base_prompter
  - _self_

llm_params:
  model_name: "llama2-7b"
  # To load from Hugging Face Hub:
  # checkpoint: 'meta-llama/Llama-2-7b-hf'
  # To load from local path:
  checkpoint: "/data/Newdisk/jiachengyu/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9"
  # Memory optimization parameters
  load_in_8bit: true
  device_map: "auto"
  offload_folder: "offload"
  max_memory: {0: "40GiB", 1: "40GiB", "cpu": "48GiB"}
  lora_params:
    warmstart: false
    lora_checkpoint: "${data.data_dir}/checkpoints/warmstart_v2" # change this path as necessary
    lora_config:
      r: 8
      lora_alpha: 16
      bias: "none"
      target_modules:
        - q_proj
        - v_proj
        - lm_head
prompt_manager:
  prompt_template: 
    - key: system_message
      msg: "<s>"
    - key: hyper_instruct
      msg: "{instruct}"  # loaded from context
    - key: suffix
      msg: "{suffix}"  # loaded from context
